# -*- coding: utf-8 -*-
"""Final Transaction MIS Cleaning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17p5sDw7-5Q1VFpPHmsH_36JcVH2SkWkd
"""

from google.colab import files
import pandas as pd
import numpy as np
import re
from openpyxl import load_workbook

# ==================================================
# ROBUST COLUMN HANDLING
# ==================================================
def normalize_col(c):
    return re.sub(r'[^a-z0-9]', '', str(c).lower())

def find_col(df, possible):
    col_map = {normalize_col(c): c for c in df.columns}
    for p in possible:
        key = normalize_col(p)
        if key in col_map:
            return col_map[key]
    raise Exception(f"Missing column. Tried {possible}\nAvailable: {list(df.columns)}")

def strip_time_from_dates(df):
    """Remove time component from all datetime columns"""
    for col in df.columns:
        if pd.api.types.is_datetime64_any_dtype(df[col]):
            df[col] = df[col].dt.date
    return df

# ==================================================
# UPLOAD FILES
# ==================================================
print("ðŸ“¤ Upload Transaction Input File")
input_file = list(files.upload().keys())[0]

print("\nðŸ“¤ Upload System Client Master File")
system_client_file = list(files.upload().keys())[0]

print("\nðŸ“¤ Upload System Scheme Master File")
system_scheme_file = list(files.upload().keys())[0]

print("\nðŸ“¤ Upload Master File")
master_file = list(files.upload().keys())[0]

# ==================================================
# UPDATE MASTER FILE - CLIENT MASTER
# ==================================================
print("\n" + "="*60)
print("ðŸ”„ UPDATING CLIENT MASTER")
print("="*60)

# Read System Client Master
system_client = pd.read_excel(system_client_file)
print(f"\nðŸ“Š System Client Master loaded: {len(system_client)} rows")
print(f"   Columns: {system_client.columns.tolist()}")

# Read Master File's Client Master sheet
master_client = pd.read_excel(master_file, sheet_name="Client Master")
print(f"\nðŸ“Š Master Client Master loaded: {len(master_client)} rows")
print(f"   Columns: {master_client.columns.tolist()}")

# Target columns we want to match/extract
target_columns = ['CLIENTID', 'CLIENTNAME', 'CLIENTCODE', 'PANNUMBER', 'GROUPNAME', 'RELMGRNAME', 'BILLGROUP']

# Normalize column names for matching
system_client_normalized = {normalize_col(c): c for c in system_client.columns}
master_client_normalized = {normalize_col(c): c for c in master_client.columns}

# Find matching columns in System Client Master
system_matched_cols = {}
for target_col in target_columns:
    normalized_target = normalize_col(target_col)
    if normalized_target in system_client_normalized:
        system_matched_cols[target_col] = system_client_normalized[normalized_target]
        print(f"   âœ… Matched: {target_col} -> {system_client_normalized[normalized_target]}")
    else:
        print(f"   âš ï¸  Not found in System: {target_col}")

# Extract only the matched columns from System Client Master
if system_matched_cols:
    system_client_filtered = system_client[list(system_matched_cols.values())].copy()
    # Rename to standard names
    system_client_filtered.columns = list(system_matched_cols.keys())
else:
    raise Exception("No matching columns found between System Client Master and target columns!")

# Clean CLIENTCODE for comparison
if 'CLIENTCODE' in system_client_filtered.columns:
    system_client_filtered['_clientcode_clean'] = (
        system_client_filtered['CLIENTCODE']
        .astype(str)
        .str.strip()
        .str.replace(".0", "", regex=False)
        .str.upper()
    )
else:
    raise Exception("CLIENTCODE not found in System Client Master!")

if 'CLIENTCODE' in master_client.columns:
    master_client['_clientcode_clean'] = (
        master_client['CLIENTCODE']
        .astype(str)
        .str.strip()
        .str.replace(".0", "", regex=False)
        .str.upper()
    )
else:
    # Try to find clientcode with different casing
    clientcode_col = None
    for col in master_client.columns:
        if normalize_col(col) == 'clientcode':
            clientcode_col = col
            break
    if clientcode_col:
        master_client['_clientcode_clean'] = (
            master_client[clientcode_col]
            .astype(str)
            .str.strip()
            .str.replace(".0", "", regex=False)
            .str.upper()
        )
    else:
        raise Exception("CLIENTCODE not found in Master Client Master!")

# Find missing records
master_clientcodes = set(master_client['_clientcode_clean'].unique())
system_clientcodes = set(system_client_filtered['_clientcode_clean'].unique())
missing_clientcodes = system_clientcodes - master_clientcodes

print(f"\nðŸ” Comparison Results:")
print(f"   System Client Master: {len(system_clientcodes)} unique client codes")
print(f"   Master Client Master: {len(master_clientcodes)} unique client codes")
print(f"   Missing in Master: {len(missing_clientcodes)} client codes")

if len(missing_clientcodes) > 0:
    # Get the missing records
    missing_records = system_client_filtered[
        system_client_filtered['_clientcode_clean'].isin(missing_clientcodes)
    ].copy()

    # Drop the temporary clean column
    missing_records = missing_records.drop(columns=['_clientcode_clean'])

    print(f"\nðŸ“‹ New Client Codes to be added:")
    for i, code in enumerate(sorted(missing_clientcodes)[:20], 1):
        print(f"   {i}. {code}")
    if len(missing_clientcodes) > 20:
        print(f"   ... and {len(missing_clientcodes) - 20} more")

    # Get master columns (excluding temp column)
    master_columns = [col for col in master_client.columns if col != '_clientcode_clean']

    # Create a mapping between standardized names and master column names
    master_col_normalized = {normalize_col(c): c for c in master_columns}

    # Create new missing_records dataframe with master column names
    missing_records_mapped = pd.DataFrame()

    for std_col in missing_records.columns:
        normalized_std = normalize_col(std_col)
        if normalized_std in master_col_normalized:
            # Map to master column name
            master_col_name = master_col_normalized[normalized_std]
            missing_records_mapped[master_col_name] = missing_records[std_col]

    # Add any master columns that don't exist in missing_records
    for col in master_columns:
        if col not in missing_records_mapped.columns:
            missing_records_mapped[col] = ""

    # Reorder to match master columns
    missing_records_mapped = missing_records_mapped[master_columns]

    print(f"\nðŸ” Column mapping verification:")
    print(f"   System columns: {list(missing_records.columns)}")
    print(f"   Master columns: {master_columns}")
    print(f"   Mapped columns: {list(missing_records_mapped.columns)}")

    # Append to master client
    master_client_updated = pd.concat([
        master_client.drop(columns=['_clientcode_clean']),
        missing_records_mapped
    ], ignore_index=True)

    print(f"\nâœ… Updated Master Client Master: {len(master_client_updated)} rows")
else:
    master_client_updated = master_client.drop(columns=['_clientcode_clean'])
    print(f"\nâœ… No new client records to add. Master is up to date!")

# ==================================================
# UPDATE MASTER FILE - SCHEME MASTER
# ==================================================
print("\n" + "="*60)
print("ðŸ”„ UPDATING SCHEME MASTER")
print("="*60)

# Read System Scheme Master (first row is header)
system_scheme = pd.read_excel(system_scheme_file, header=0)
print(f"\nðŸ“Š System Scheme Master loaded: {len(system_scheme)} rows")
print(f"   Columns: {system_scheme.columns.tolist()}")

# Read Master File's Scheme Master sheet (skip first row, row 2 is header)
master_scheme = pd.read_excel(master_file, sheet_name="Scheme Master", header=1)
print(f"\nðŸ“Š Master Scheme Master loaded: {len(master_scheme)} rows")
print(f"   Columns: {master_scheme.columns.tolist()}")

# Column mapping: System -> Master
scheme_column_mapping = {
    'SYMBOLID': 'SYMBOLID',
    'SYMBOLNAME': 'Scheme name',
    'ISINCODE': 'ISIN',
    'REFSYMBOL5': 'Symbolcode5',
    'DIMNAME15': 'DIMNAME15 Old',
    'ASTCLSNAME': 'ASTCLSNAME',
    'DIMNAME13': 'DIMNAME13'
}

# Normalize column names for matching
system_scheme_normalized = {normalize_col(c): c for c in system_scheme.columns}

# Find matching columns in System Scheme Master
system_scheme_matched_cols = {}
for system_col, master_col in scheme_column_mapping.items():
    normalized_system = normalize_col(system_col)
    if normalized_system in system_scheme_normalized:
        system_scheme_matched_cols[master_col] = system_scheme_normalized[normalized_system]
        print(f"   âœ… Matched: {system_col} -> {system_scheme_normalized[normalized_system]}")
    else:
        print(f"   âš ï¸  Not found in System Scheme Master: {system_col}")

# Extract and rename columns from System Scheme Master
if system_scheme_matched_cols:
    system_scheme_filtered = system_scheme[list(system_scheme_matched_cols.values())].copy()
    # Rename to master column names
    system_scheme_filtered.columns = list(system_scheme_matched_cols.keys())
else:
    raise Exception("No matching columns found between System Scheme Master and target columns!")

# Clean SYMBOLID for comparison
if 'SYMBOLID' in system_scheme_filtered.columns:
    system_scheme_filtered['_symbolid_clean'] = (
        system_scheme_filtered['SYMBOLID']
        .astype(str)
        .str.strip()
        .str.upper()
    )
else:
    raise Exception("SYMBOLID not found in System Scheme Master!")

if 'SYMBOLID' in master_scheme.columns:
    master_scheme['_symbolid_clean'] = (
        master_scheme['SYMBOLID']
        .astype(str)
        .str.strip()
        .str.upper()
    )
else:
    # Try to find symbolid with different casing
    symbolid_col = None
    for col in master_scheme.columns:
        if normalize_col(col) == 'symbolid':
            symbolid_col = col
            break
    if symbolid_col:
        master_scheme['_symbolid_clean'] = (
            master_scheme[symbolid_col]
            .astype(str)
            .str.strip()
            .str.upper()
        )
    else:
        raise Exception("SYMBOLID not found in Master Scheme Master!")

# Find missing records
master_symbolids = set(master_scheme['_symbolid_clean'].unique())
system_symbolids = set(system_scheme_filtered['_symbolid_clean'].unique())
missing_symbolids = system_symbolids - master_symbolids

print(f"\nðŸ” Comparison Results:")
print(f"   System Scheme Master: {len(system_symbolids)} unique symbol IDs")
print(f"   Master Scheme Master: {len(master_symbolids)} unique symbol IDs")
print(f"   Missing in Master: {len(missing_symbolids)} symbol IDs")

if len(missing_symbolids) > 0:
    # Get the missing records
    missing_scheme_records = system_scheme_filtered[
        system_scheme_filtered['_symbolid_clean'].isin(missing_symbolids)
    ].copy()

    # Drop the temporary clean column
    missing_scheme_records = missing_scheme_records.drop(columns=['_symbolid_clean'])

    print(f"\nðŸ“‹ New Symbol IDs to be added:")
    for i, code in enumerate(sorted(missing_symbolids)[:20], 1):
        print(f"   {i}. {code}")
    if len(missing_symbolids) > 20:
        print(f"   ... and {len(missing_symbolids) - 20} more")

    # Get master columns (excluding temp column)
    master_scheme_columns = [col for col in master_scheme.columns if col != '_symbolid_clean']

    # Add the three blank columns for new records
    missing_scheme_records['DIMNAME15 - New'] = ""
    missing_scheme_records['ASTCLSNAME New'] = ""
    missing_scheme_records['Manufacturer Name'] = ""

    # Ensure all master columns exist
    for col in master_scheme_columns:
        if col not in missing_scheme_records.columns:
            missing_scheme_records[col] = ""

    # Reorder to match master columns
    missing_scheme_records = missing_scheme_records[master_scheme_columns]

    print(f"\nðŸ” Column mapping verification:")
    print(f"   Mapped columns: {list(missing_scheme_records.columns)}")

    # Append to master scheme
    master_scheme_updated = pd.concat([
        master_scheme.drop(columns=['_symbolid_clean']),
        missing_scheme_records
    ], ignore_index=True)

    print(f"\nâœ… Updated Master Scheme Master: {len(master_scheme_updated)} rows")
else:
    master_scheme_updated = master_scheme.drop(columns=['_symbolid_clean'])
    print(f"\nâœ… No new scheme records to add. Master is up to date!")

# ==================================================
# SAVE UPDATED MASTER FILE
# ==================================================
print(f"\nðŸ’¾ Updating Master File with new Client Master and Scheme Master data...")

# Load the workbook
wb = load_workbook(master_file)

# Update Client Master sheet
if "Client Master" in wb.sheetnames:
    del wb["Client Master"]

from openpyxl.utils.dataframe import dataframe_to_rows
ws_client = wb.create_sheet("Client Master", 0)  # Insert at the beginning

for r in dataframe_to_rows(master_client_updated, index=False, header=True):
    ws_client.append(r)

# Update Scheme Master sheet
if "Scheme Master" in wb.sheetnames:
    del wb["Scheme Master"]

ws_scheme = wb.create_sheet("Scheme Master", 1)  # Insert after Client Master

# CRITICAL FIX: Add BOTH header rows from original file
# Read the original first header row (row 0, before header=1)
original_scheme_headers = pd.read_excel(master_file, sheet_name="Scheme Master", nrows=1, header=None)

# Add the first header row (descriptive headers from Row 1)
ws_scheme.append(original_scheme_headers.iloc[0].tolist())

# Add the second header row (matching headers from Row 2) and data
for r in dataframe_to_rows(master_scheme_updated, index=False, header=True):
    ws_scheme.append(r)

# Save the updated master file
updated_master_file = "Updated_Master_File.xlsx"
wb.save(updated_master_file)
print(f"âœ… Master File updated and saved as: {updated_master_file}")

# Now use the updated master file for the rest of the process
master_file = updated_master_file

print("="*60 + "\n")

# ==================================================
# READ INPUT FILE
# ==================================================
df = pd.read_excel(input_file)
base_rows = len(df)
original_cols = df.columns.tolist()

# Strip time from any date columns in input
df = strip_time_from_dates(df)

# ==================================================
# IDENTIFY INPUT COLUMNS (ROBUST)
# ==================================================
client_col = find_col(df, ["client name"])
ws_col     = find_col(df, ["ws account code"])
sec_col    = find_col(df, ["security code"])
trf_col    = find_col(df, ["trfamt", "transfer amount"])
net_col    = find_col(df, ["net amount", "amount"])
txn_col    = find_col(df, ["tran desc", "transaction description"])
desc_col   = find_col(df, ["descmemo", "desc memo", "description memo"])

# ==================================================
# BASE COLUMNS
# ==================================================
df["Length"] = df[ws_col].astype(str).str.len()
df["Del Tag"] = ""
df["Ambit First"] = ""

# ==================================================
# STEP 1: MAP AMBIT FIRST
# ==================================================
print("\nðŸ”¹ STEP 1: Mapping Ambit First...")
ambit_first = pd.read_excel(master_file, sheet_name="Ambit First")
ambit_first.columns = ambit_first.columns.astype(str).str.strip().str.lower().str.replace(" ", "_")

# Check if clientcode column exists
if "clientcode" not in ambit_first.columns:
    raise Exception(f"CLIENTCODE not found in Ambit First sheet. Found columns: {ambit_first.columns.tolist()}")

# Clean WS Account Code (remove .0 that comes from numeric values in Excel)
df["_ws_clean"] = df[ws_col].astype(str).str.strip().str.replace(".0", "", regex=False)

# Clean Client Code from Ambit First sheet
ambit_first["_client_clean"] = ambit_first["clientcode"].astype(str).str.strip().str.replace(".0", "", regex=False)
ambit_first = ambit_first.drop_duplicates(subset=["_client_clean"])

# Create set for faster lookup
ambit_set = set(ambit_first["_client_clean"])

# Tag matching rows as "Ambit First"
matches = df["_ws_clean"].isin(ambit_set)
df.loc[matches, "Ambit First"] = "Ambit First"
print(f"   âœ… {matches.sum()} rows tagged as 'Ambit First'")

# Keep _ws_clean for later use
df["_ws"] = df["_ws_clean"]

# ==================================================
# STEP 2: DEL TAG LOGIC (IN SPECIFIC ORDER)
# ==================================================
print("\nðŸ”¹ STEP 2: Applying Del Tag logic...")

# Del PAN: Length = 10
pan_condition = (df["Del Tag"] == "") & (df["Length"] == 10)
df.loc[pan_condition, "Del Tag"] = "Del PAN"
print(f"   Del PAN: {pan_condition.sum()} rows")

# Del PMS: No Del Tag AND No Ambit First AND WS Code starts with ND/DS/DM
pms_condition = (
    (df["Del Tag"] == "") &
    (df["Ambit First"] == "") &
    df["_ws"].str.upper().str.startswith(("ND", "DS", "DM"))
)
df.loc[pms_condition, "Del Tag"] = "Del PMS"
print(f"   Del PMS: {pms_condition.sum()} rows")

# Del AWPL: Client Name contains "ambit wealth"
awpl_condition = (
    (df["Del Tag"] == "") &
    df[client_col].str.contains("ambit wealth", case=False, na=False)
)
df.loc[awpl_condition, "Del Tag"] = "Del AWPL"
print(f"   Del AWPL: {awpl_condition.sum()} rows")

# Del AFPL: Client Name contains "Ambit Finvest Private Limited"
afpl_condition = (
    (df["Del Tag"] == "") &
    df[client_col].str.contains("Ambit Finvest Private Limited", case=False, na=False)
)
df.loc[afpl_condition, "Del Tag"] = "Del AFPL"
print(f"   Del AFPL: {afpl_condition.sum()} rows")

# Del Dummy: Client Name contains "dummy"
dummy_condition = (
    (df["Del Tag"] == "") &
    df[client_col].str.contains("dummy", case=False, na=False)
)
df.loc[dummy_condition, "Del Tag"] = "Del Dummy"
print(f"   Del Dummy: {dummy_condition.sum()} rows")

# Del Cash: Security Code contains "cash"
cash_condition = (
    (df["Del Tag"] == "") &
    df[sec_col].str.contains("cash", case=False, na=False)
)
df.loc[cash_condition, "Del Tag"] = "Del Cash"
print(f"   Del Cash: {cash_condition.sum()} rows")

# Del TDSAccount: Security Code contains "tds"
tds_condition = (
    (df["Del Tag"] == "") &
    df[sec_col].str.contains("tds", case=False, na=False)
)
df.loc[tds_condition, "Del Tag"] = "Del TDSAccount"
print(f"   Del TDSAccount: {tds_condition.sum()} rows")

# Del MFApplication: Security Code contains "mfapplication"
mfapp_condition = (
    (df["Del Tag"] == "") &
    df[sec_col].str.contains("mfapplication", case=False, na=False)
)
df.loc[mfapp_condition, "Del Tag"] = "Del MFApplication"
print(f"   Del MFApplication: {mfapp_condition.sum()} rows")

# Del INTACCPUR: Security Code contains "intaccpur"
intaccpur_condition = (
    (df["Del Tag"] == "") &
    df[sec_col].str.contains("intaccpur", case=False, na=False)
)
df.loc[intaccpur_condition, "Del Tag"] = "Del INTACCPUR"
print(f"   Del INTACCPUR: {intaccpur_condition.sum()} rows")

# Summary
blank_del_count = (df["Del Tag"] == "").sum()
ambit_first_count = (df["Ambit First"] == "Ambit First").sum()
print(f"\n   ðŸ“Š Summary:")
print(f"      - {len(df) - blank_del_count} rows tagged with Del Tag")
print(f"      - {ambit_first_count} rows tagged as Ambit First")
print(f"      - {blank_del_count} rows remain blank (for Final Data)")

# ==================================================
# TRNX TYPE UPDATE
# ==================================================
tt = pd.read_excel(master_file, sheet_name="Trnx Type Update")

# Keep original column names but strip spaces
tt.columns = [str(c).strip() for c in tt.columns]

# Get first column name (Tran Desc column)
tran_desc_col = tt.columns[0]

# Get Replace column name (should be column B, index 1)
replace_col = tt.columns[1] if len(tt.columns) > 1 else None

# Remove duplicates based on Tran Desc column
tt = tt.drop_duplicates(subset=[tran_desc_col])

# Create Replace map with exact matching (case-sensitive, space-sensitive as per Excel VLOOKUP)
if replace_col:
    replace_map = dict(zip(
        tt[tran_desc_col].astype(str).str.strip(),
        tt[replace_col].astype(str).str.strip()
    ))
else:
    replace_map = {}

# Delete map: Column E (This Type Delete) - value itself
delete_col = tt.columns[4] if len(tt.columns) > 4 else None
if delete_col:
    delete_lookup = set(tt[delete_col].dropna().astype(str).str.strip())
else:
    delete_lookup = set()

# Clean Tran Desc in input for matching (strip spaces only)
df['_txn_clean'] = df[txn_col].astype(str).str.strip()

# Revised Trnx Amount: Use Transfer Amount if > 1, else Net Amount
df["Revised Trnx Amount"] = np.where(
    pd.to_numeric(df[trf_col], errors="coerce") > 1,
    pd.to_numeric(df[trf_col], errors="coerce"),
    pd.to_numeric(df[net_col], errors="coerce")
)

# ==================================================
# DEBUG: Show missing mappings
# ==================================================
input_txn_values = set(df['_txn_clean'].unique())
master_txn_values = set(replace_map.keys())
missing_values = input_txn_values - master_txn_values

print(f"\nðŸ” DEBUG: Transaction Descriptions NOT found in Master 'Trnx Type Update' sheet:")
if missing_values:
    for val in sorted(missing_values):
        if val and val != 'nan':
            count = (df['_txn_clean'] == val).sum()
            print(f"   '{val}' - appears {count} times")
else:
    print("   âœ… All transaction descriptions found in master!")

# ==================================================
# CONSIDER COLUMN LOGIC
# ==================================================
# IF DESCMEMO = "Broker Change" AND Tran Desc = "InFlow" THEN "AUM Trf In"
# ELSE VLOOKUP Tran Desc in Trnx Type Update sheet and bring Replace value

# Check for Broker Change AND InFlow condition
broker_inflow_condition = (
    (df[desc_col].astype(str).str.strip() == "Broker Change") &
    (df[txn_col].astype(str).str.strip() == "InFlow")
)

# Apply logic
df["Consider"] = df['_txn_clean'].map(replace_map).fillna("")
df.loc[broker_inflow_condition, "Consider"] = "AUM Trf In"

# ==================================================
# DELETE COLUMN LOGIC
# ==================================================
# VLOOKUP Tran Desc in Column E of Trnx Type Update
# If found, return the value itself
df["Delete"] = df['_txn_clean'].apply(
    lambda x: x if x in delete_lookup else ""
)

# ==================================================
# TRANS TYPE 2 & SALES COLUMNS
# ==================================================
# Trans Type 2: Copy of Consider
df["Trans Type 2"] = df["Consider"]

# Gross Sales: "Gross Sales" if Purchase/AUM Trf In/Switch In/SIP, else "Redemption"
df["Gross Sales"] = np.where(
    df["Trans Type 2"].isin(["Purchase", "AUM Trf In", "Switch In", "SIP"]),
    "Gross Sales", "Redemption"
)

# Net Sales: "Net Sales" if in specified list, else "0"
df["Net Sales"] = np.where(
    df["Trans Type 2"].isin([
        "Purchase", "AUM Trf In", "Switch In", "SIP",
        "Redemption", "AUM Trf Out", "Switch Out", "SWP"
    ]),
    "Net Sales", "0"
)

# ==================================================
# AMT IN CRS - NEW LOGIC BASED ON GROSS SALES COLUMN
# ==================================================
# If Gross Sales = "Gross Sales" -> Revised Amount / 1e7 (positive)
# If Gross Sales = "Redemption" -> -(Revised Amount / 1e7) (negative)

def calculate_amt_in_crs(row):
    revised_amt = row["Revised Trnx Amount"]
    gross_sales_val = str(row["Gross Sales"]).strip()

    if pd.isna(revised_amt):
        return 0

    amt_in_crs = revised_amt / 1e7

    # If Gross Sales is "Redemption", make it negative
    if gross_sales_val == "Redemption":
        amt_in_crs = -amt_in_crs

    return amt_in_crs

df["Amt in Crs"] = df.apply(calculate_amt_in_crs, axis=1)

# ==================================================
# SCHEME MASTER
# ==================================================
scheme = pd.read_excel(master_file, sheet_name="Scheme Master", header=1)
scheme.columns = scheme.columns.str.lower().str.strip()
scheme = scheme.drop_duplicates(subset=["symbolid"])

df = df.merge(
    scheme[[
        "symbolid",
        "dimname15 - new",
        "astclsname new",
        "dimname13",
        "manufacturer name"
    ]],
    how="left",
    left_on=sec_col,
    right_on="symbolid"
)

df.rename(columns={
    "dimname15 - new": "Product New",
    "astclsname new": "Asset Class New",
    "dimname13": "Product Category New",
    "manufacturer name": "Manufacturer Name New"
}, inplace=True)

df.drop(columns=["symbolid"], inplace=True)

# ==================================================
# AMBIT FIRST OVERRIDE FOR SCHEME MASTER COLUMNS
# ==================================================
print("\nðŸ”¹ Applying Ambit First overrides for Scheme Master columns...")

ambit_first_mask = (df["Ambit First"] == "Ambit First")
ambit_override_count = ambit_first_mask.sum()

if ambit_override_count > 0:
    df.loc[ambit_first_mask, "Product New"] = "GPC - PMS"
    df.loc[ambit_first_mask, "Asset Class New"] = "Other NDPMS"
    df.loc[ambit_first_mask, "Product Category New"] = "Equity PMS"
    df.loc[ambit_first_mask, "Manufacturer Name New"] = "GPC - Ambit First"
    print(f"   âœ… {ambit_override_count} rows overridden with Ambit First values")
else:
    print(f"   â„¹ï¸  No Ambit First rows to override")

# ==================================================
# CLIENT MASTER - ENHANCED WITH DIAGNOSTICS
# ==================================================
print("\n" + "="*60)
print("CLIENT MASTER LOOKUP - ENHANCED")
print("="*60)

client = pd.read_excel(master_file, sheet_name="Client Master")
client.columns = client.columns.str.lower().str.strip().str.replace(" ", "_")

print(f"ðŸ“Š Client Master loaded: {len(client)} rows")
print(f"   Columns: {client.columns.tolist()}")

# Enhanced cleaning for clientcode
client["_client_clean"] = (
    client["clientcode"]
    .astype(str)
    .str.strip()
    .str.replace(".0", "", regex=False)
    .str.upper()  # Convert to uppercase for case-insensitive matching
)

# Count duplicates before dropping
duplicates_count = client["_client_clean"].duplicated().sum()
if duplicates_count > 0:
    print(f"âš ï¸  Found {duplicates_count} duplicate ClientCodes (keeping last occurrence)")

# Remove duplicates - keep LAST occurrence (often more recent)
client = client.drop_duplicates(subset=["_client_clean"], keep="last")

print(f"âœ… After deduplication: {len(client)} unique client codes")

# Also clean and uppercase the WS code in main df for matching
df["_ws_upper"] = df["_ws"].str.upper()

# Show sample of what we're matching
print(f"\nðŸ“‹ Sample WS Codes from transaction file (first 5):")
for ws in df["_ws_upper"].unique()[:5]:
    print(f"   '{ws}'")

print(f"\nðŸ“‹ Sample ClientCodes from master (first 5):")
for code in client["_client_clean"].unique()[:5]:
    print(f"   '{code}'")

# Perform the merge
df = df.merge(
    client[["_client_clean", "groupname", "pannumber", "relmgrname"]],
    how="left",
    left_on="_ws_upper",
    right_on="_client_clean"
)

df.rename(columns={
    "groupname": "Family Name as per Client Master",
    "pannumber": "Pan No",
    "relmgrname": "Banker Name"
}, inplace=True)

# ==================================================
# DIAGNOSTIC: CHECK BANKER NAME LOOKUP RESULTS
# ==================================================
print("\n" + "="*60)
print("ðŸ” BANKER NAME LOOKUP DIAGNOSTICS")
print("="*60)

# Find rows with blank Banker Name
missing_banker = df[df["Banker Name"].isna() | (df["Banker Name"] == "")]
missing_count = len(missing_banker)

print(f"âœ… Successful matches: {len(df) - missing_count} out of {len(df)} rows ({(len(df) - missing_count)/len(df)*100:.1f}%)")
print(f"âŒ Missing Banker Name: {missing_count} out of {len(df)} rows ({missing_count/len(df)*100:.1f}%)")

if missing_count > 0:
    missing_ws = missing_banker["_ws_upper"].unique()
    print(f"\nðŸ“‹ Unique WS Codes with missing Banker Name: {len(missing_ws)}")
    print(f"   Showing first 10:")
    for ws in missing_ws[:10]:
        count = (missing_banker["_ws_upper"] == ws).sum()
        print(f"      '{ws}' - appears {count} times")

    client_codes_set = set(client["_client_clean"])
    not_in_master = [ws for ws in missing_ws if ws not in client_codes_set]

    print(f"\nðŸ”Ž Analysis:")
    print(f"   WS Codes NOT found in Client Master: {len(not_in_master)} out of {len(missing_ws)}")
    if not_in_master:
        print(f"   Examples of WS codes missing from Client Master:")
        for ws in not_in_master[:5]:
            print(f"      '{ws}'")

    in_master_but_failed = [ws for ws in missing_ws if ws in client_codes_set]
    if in_master_but_failed:
        print(f"\nâš ï¸  WARNING: {len(in_master_but_failed)} WS codes exist in master but still failed to match!")
        print(f"   This indicates a data quality issue. Examples:")
        for ws in in_master_but_failed[:3]:
            print(f"      '{ws}'")

print("="*60 + "\n")

# Clean up temporary columns
df.drop(columns=["_client_clean", "_ws_upper"], inplace=True, errors='ignore')

# ==================================================
# EMPLOYEE MAPPING MASTER
# ==================================================
emp = pd.read_excel(master_file, sheet_name="Employee Mapping Master")
emp.columns = emp.columns.str.lower().str.strip().str.replace(" ", "_")
emp = emp.drop_duplicates(subset=["banker_name"])

df = df.merge(
    emp[["banker_name", "banker_name_new", "banker_group_name", "group_tag"]],
    how="left",
    left_on="Banker Name",
    right_on="banker_name"
)

df.rename(columns={
    "banker_name_new": "Banker Name New",
    "banker_group_name": "Banker Group Name",
    "group_tag": "Banker Group Tag"
}, inplace=True)

df.drop(columns=["banker_name"], inplace=True)

# ==================================================
# NTB DATA
# ==================================================
ntb = pd.read_excel(master_file, sheet_name="NTB Data")
ntb.columns = ntb.columns.str.lower().str.strip().str.replace(" ", "_")
ntb = ntb.drop_duplicates(subset=["family_name"])

df = df.merge(
    ntb[["family_name", "month", "fy"]],
    how="left",
    left_on="Family Name as per Client Master",
    right_on="family_name"
)

df.rename(columns={"month": "NTB Month", "fy": "NTB FY"}, inplace=True)
df.drop(columns=["family_name"], inplace=True)

# ==================================================
# CREATE BLANK COLUMNS
# ==================================================
df["Family Name Final"] = ""
df["Pk Remark"] = ""
df["Extra column"] = ""
df["Month-New"] = ""
df["YTD Tag"] = ""
df["Month-New For Banker MIS"] = ""

# ==================================================
# DEFINE FINAL COLUMN ORDER - SHUFFLED
# ==================================================
final_column_order = [
    *original_cols,
    "Revised Trnx Amount",
    "Consider",
    "Delete",
    "Trans Type 2",
    "Gross Sales",
    "Net Sales",
    "Product New",
    "Asset Class New",
    "Product Category New",
    "Manufacturer Name New",
    "Banker Name",
    "Banker Name New",
    "Banker Group Name",
    "Banker Group Tag",
    "Amt in Crs",
    "Family Name as per Client Master",
    "Family Name Final",
    "Pk Remark",
    "Extra column",
    "Month-New",
    "YTD Tag",
    "Ambit First",
    "Pan No",
    "Month-New For Banker MIS",
    "NTB Month",
    "NTB FY",
    "Length",
    "Del Tag"
]

final_column_order = [col for col in final_column_order if col in df.columns]
df = df[final_column_order]

# ==================================================
# FINAL SAFETY CHECK
# ==================================================
assert len(df) == base_rows, f"Row mismatch! Input={base_rows}, Output={len(df)}"

# ==================================================
# CREATE SHEETS
# ==================================================
df_working = df[df["Del Tag"] == ""].copy()

print("\n" + "="*60)
print("ðŸ“Š WORKING SHEET COMPOSITION")
print("="*60)
print(f"Total rows in Working sheet: {len(df_working)}")
print(f"   - Rows with Ambit First: {(df_working['Ambit First'] == 'Ambit First').sum()}")
print(f"   - Rows without Ambit First: {(df_working['Ambit First'] == '').sum()}")
print("="*60 + "\n")

df_final = df_working[
    (df_working["Consider"].notna()) &
    (df_working["Consider"] != "") &
    (df_working["Delete"].isna() | (df_working["Delete"] == ""))
].copy()

print("\n" + "="*60)
print("ðŸ“Š FINAL SHEET COMPOSITION")
print("="*60)
print(f"Total rows in Final sheet: {len(df_final)}")
print(f"   - Rows with Consider value: {(df_final['Consider'] != '').sum()}")
print(f"   - Rows with blank Delete: {(df_final['Delete'].isna() | (df_final['Delete'] == '')).sum()}")
print("="*60 + "\n")

# ==================================================
# OUTPUT FILES
# ==================================================
output_file = "Transaction_MIS_Final.xlsx"

with pd.ExcelWriter(output_file, engine='openpyxl', date_format='DD-MM-YYYY') as writer:
    df.to_excel(writer, sheet_name='Raw Dump', index=False)
    df_working.to_excel(writer, sheet_name='Working', index=False)
    df_final.to_excel(writer, sheet_name='Final', index=False)

files.download(output_file)

print(f"\nâœ… Transaction MIS Output file created: {output_file}")
print(f"   Sheet 1 'Raw Dump': {len(df)} rows")
print(f"   Sheet 2 'Working': {len(df_working)} rows")
print(f"   Sheet 3 'Final': {len(df_final)} rows")

print(f"\nðŸ’¾ Downloading Updated Master File...")
files.download(updated_master_file)

print(f"\nâœ… Updated Master File downloaded: {updated_master_file}")
print(f"\n" + "="*60)
print("ðŸŽ‰ PROCESS COMPLETED SUCCESSFULLY!")
print("="*60)
print(f"ðŸ“¥ Files Downloaded:")
print(f"   1. {output_file} (Transaction MIS with 3 sheets)")
print(f"   2. {updated_master_file} (Master with updated Client & Scheme Master)")
print(f"\nðŸ“Š Update Summary:")
print(f"   Client Master:")
print(f"      - New clients added: {len(missing_clientcodes)}")
if len(missing_clientcodes) > 0:
    print(f"      - Total clients: {len(master_client_updated)}")
print(f"   Scheme Master:")
print(f"      - New schemes added: {len(missing_symbolids)}")
if len(missing_symbolids) > 0:
    print(f"      - Total schemes: {len(master_scheme_updated)}")
print("="*60)